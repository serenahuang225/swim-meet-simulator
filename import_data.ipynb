{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "7fa1d9e6",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "\n",
        "with open(\"MSHSAA Swimming Performance List.html\", \"r\", encoding=\"utf-8\") as f:\n",
        "    soup = BeautifulSoup(f, \"html.parser\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18a70cd7",
      "metadata": {},
      "source": [
        "# analyze HTML structure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "4e07319c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of tables: 13\n"
          ]
        }
      ],
      "source": [
        "# Find tables\n",
        "tables = soup.find_all('table')\n",
        "print(f\"Number of tables: {len(tables)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "bb86bc3e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Elements with class 'fs_grid': 11\n"
          ]
        }
      ],
      "source": [
        "# Look for specific class patterns\n",
        "fs_grid_elements = soup.find_all(class_='fs_grid')\n",
        "print(f\"Elements with class 'fs_grid': {len(fs_grid_elements)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "1b29ef02",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- fs_grid element 1 ---\n",
            "Tag: table\n",
            "Contains 90 rows\n",
            "\n",
            "Row 0:\n",
            "  td[0]: Rank...\n",
            "  td[1]: School...\n",
            "  td[2]: Contest...\n",
            "  td[3]: Date...\n",
            "  td[4]: Time...\n",
            "\n",
            "Row 1:\n",
            "  td[0]: #1...\n",
            "  td[1]: St. Joseph's Academy...\n",
            "  td[2]: COMO Invite...\n",
            "  td[3]: 1/9/26...\n",
            "  td[4]: 1:46.57...\n",
            "\n",
            "Row 2:\n",
            "  td[0]: #2...\n",
            "  td[1]: Park Hill South...\n",
            "  td[2]: COMO Invitational...\n",
            "  td[3]: 1/9/26...\n",
            "  td[4]: 1:48.32...\n",
            "--- fs_grid element 2 ---\n",
            "Tag: table\n",
            "Contains 180 rows\n",
            "\n",
            "Row 0:\n",
            "  td[0]: Rank...\n",
            "  td[1]: Student...\n",
            "  td[2]: Grade...\n",
            "  td[3]: School...\n",
            "  td[4]: Contest...\n",
            "  td[5]: Date...\n",
            "  td[6]: Time...\n",
            "\n",
            "Row 1:\n",
            "  td[0]: #1...\n",
            "  td[1]: Helena Tietjen\n",
            "Pembroke Hill...\n",
            "  td[2]: 12...\n",
            "  td[3]: Pembroke Hill...\n",
            "  td[4]: COMO Invitational...\n",
            "  td[5]: 1/9/26...\n",
            "  td[6]: 1:48.61...\n",
            "\n",
            "Row 2:\n",
            "  td[0]: #2...\n",
            "  td[1]: Anna-Grace Guenther\n",
            "St. Joseph's Academy...\n",
            "  td[2]: 12...\n",
            "  td[3]: St. Joseph's Academy...\n",
            "  td[4]: COMO Invite...\n",
            "  td[5]: 1/9/26...\n",
            "  td[6]: 1:51.91...\n",
            "--- fs_grid element 3 ---\n",
            "Tag: table\n",
            "Contains 153 rows\n",
            "\n",
            "Row 0:\n",
            "  td[0]: Rank...\n",
            "  td[1]: Student...\n",
            "  td[2]: Grade...\n",
            "  td[3]: School...\n",
            "  td[4]: Contest...\n",
            "  td[5]: Date...\n",
            "  td[6]: Time...\n",
            "\n",
            "Row 1:\n",
            "  td[0]: #1...\n",
            "  td[1]: Charlotte Brown\n",
            "Villa Duchesne...\n",
            "  td[2]: 12...\n",
            "  td[3]: Villa Duchesne...\n",
            "  td[4]: COMO Meet...\n",
            "  td[5]: 1/9/26...\n",
            "  td[6]: 2:04.27...\n",
            "\n",
            "Row 2:\n",
            "  td[0]: #1...\n",
            "  td[1]: Helena Tietjen\n",
            "Pembroke Hill...\n",
            "  td[2]: 12...\n",
            "  td[3]: Pembroke Hill...\n",
            "  td[4]: KC Classic...\n",
            "  td[5]: 1/16/26...\n",
            "  td[6]: 2:04.83...\n"
          ]
        }
      ],
      "source": [
        "# Check what's inside fs_grid elements\n",
        "for i, element in enumerate(fs_grid_elements[:3]):\n",
        "    print(f\"--- fs_grid element {i+1} ---\")\n",
        "    print(f\"Tag: {element.name}\")\n",
        "    print(f\"Contains {len(element.find_all('tr'))} rows\")\n",
        "    \n",
        "    # Look at first few rows\n",
        "    rows = element.find_all('tr')\n",
        "    for j, row in enumerate(rows[:3]):\n",
        "        print(f\"\\nRow {j}:\")\n",
        "        tds = row.find_all('td')\n",
        "        for k, td in enumerate(tds):\n",
        "            print(f\"  td[{k}]: {td.text.strip()[:50]}...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca53f31a",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "9c9317f7",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "MSHSAA Swimming Performance Scraper\n",
            "================================================================================\n",
            "Processing: MSHSAA Swimming Performance List.html\n",
            "Output: swimming_performance.csv\n",
            "\n",
            "Event List (11 events):\n",
            "   1. 200 Medley Relay     (Relay)\n",
            "   2. 200 Free             (Individual)\n",
            "   3. 200 IM               (Individual)\n",
            "   4. 50 Free              (Individual)\n",
            "   5. 100 Fly              (Individual)\n",
            "   6. 100 Free             (Individual)\n",
            "   7. 500 Free             (Individual)\n",
            "   8. 200 Free Relay       (Relay)\n",
            "   9. 100 Back             (Individual)\n",
            "  10. 100 Breast           (Individual)\n",
            "  11. 400 Free Relay       (Relay)\n",
            "\n",
            "================================================================================\n",
            "Starting scrape...\n",
            "Found 11 event tables\n",
            "Expected 11 events based on your list\n",
            "\n",
            "Processing 200 Medley Relay (table 1/11)\n",
            "  Table has 90 rows (including header)\n",
            "\n",
            "Processing 200 Free (table 2/11)\n",
            "  Table has 180 rows (including header)\n",
            "\n",
            "Processing 200 IM (table 3/11)\n",
            "  Table has 153 rows (including header)\n",
            "\n",
            "Processing 50 Free (table 4/11)\n",
            "  Table has 170 rows (including header)\n",
            "\n",
            "Processing 100 Fly (table 5/11)\n",
            "  Table has 145 rows (including header)\n",
            "\n",
            "Processing 100 Free (table 6/11)\n",
            "  Table has 202 rows (including header)\n",
            "\n",
            "Processing 500 Free (table 7/11)\n",
            "  Table has 146 rows (including header)\n",
            "\n",
            "Processing 200 Free Relay (table 8/11)\n",
            "  Table has 102 rows (including header)\n",
            "\n",
            "Processing 100 Back (table 9/11)\n",
            "  Table has 132 rows (including header)\n",
            "\n",
            "Processing 100 Breast (table 10/11)\n",
            "  Table has 128 rows (including header)\n",
            "\n",
            "Processing 400 Free Relay (table 11/11)\n",
            "  Table has 80 rows (including header)\n",
            "\n",
            "Scraped 1517 total records\n",
            "Saved to swimming_performance.csv\n",
            "\n",
            "=== Event Count Verification ===\n",
            "Records per event:\n",
            "  200 Medley Relay     (Relay)        89 records\n",
            "  200 Free             (Individual)  179 records\n",
            "  200 IM               (Individual)  152 records\n",
            "  50 Free              (Individual)  169 records\n",
            "  100 Fly              (Individual)  144 records\n",
            "  100 Free             (Individual)  201 records\n",
            "  500 Free             (Individual)  145 records\n",
            "  200 Free Relay       (Relay)       101 records\n",
            "  100 Back             (Individual)  131 records\n",
            "  100 Breast           (Individual)  127 records\n",
            "  400 Free Relay       (Relay)        79 records\n",
            "\n",
            "Total records across all events: 1517\n",
            "\n",
            "=== Sample Data from Each Event ===\n",
            "\n",
            "200 Medley Relay:\n",
            "--------------------------------------------------------------------------------\n",
            "  Rank   1 | St. Joseph's Academy           | St. Joseph's Academy |  1:46.57 [Relay]\n",
            "  Rank   2 | Park Hill South                | Park Hill South      |  1:48.32 [Relay]\n",
            "  Rank   3 | Rock Bridge                    | Rock Bridge          |  1:48.68 [Relay]\n",
            "  Rank   1 | Visitation Academy             | Visitation Academy   |  1:50.53 [Relay]\n",
            "  Rank   4 | Kirkwood                       | Kirkwood             |  1:50.84 [Relay]\n",
            "\n",
            "200 Free:\n",
            "--------------------------------------------------------------------------------\n",
            "  Rank   1 | Helena Tietjen                 | Pembroke Hill        |  1:48.61 [Individual]\n",
            "  Rank   2 | Anna-Grace Guenther            | St. Joseph's Academy |  1:51.91 [Individual]\n",
            "  Rank   3 | Avery Hogue                    | Park Hill South      |  1:52.28 [Individual]\n",
            "  Rank   4 | Alexis Cook                    | Parkway South        |  1:53.66 [Individual]\n",
            "  Rank   1 | Quinn Porter                   | Webster Groves       |  1:54.20 [Individual]\n",
            "\n",
            "200 IM:\n",
            "--------------------------------------------------------------------------------\n",
            "  Rank   1 | Charlotte Brown                | Villa Duchesne       |  2:04.27 [Individual]\n",
            "  Rank   1 | Helena Tietjen                 | Pembroke Hill        |  2:04.83 [Individual]\n",
            "  Rank   2 | Avery Hogue                    | Park Hill South      |  2:05.60 [Individual]\n",
            "  Rank   3 | Madison Shryock                | Rock Bridge          |  2:07.78 [Individual]\n",
            "  Rank   4 | Sarah Miner                    | Rock Bridge          |  2:08.92 [Individual]\n",
            "\n",
            "50 Free:\n",
            "--------------------------------------------------------------------------------\n",
            "  Rank   1 | Karsynne McAlister             | Central (Cape Girard |    23.77 [Individual]\n",
            "  Rank   2 | Avery Frick                    | St. Michael the Arch |    23.81 [Individual]\n",
            "  Rank   1 | Paige Blom                     | Marquette            |    23.90 [Individual]\n",
            "  Rank   2 | Esther Borgmeyer               | Kirkwood             |    24.27 [Individual]\n",
            "  Rank   3 | Ava Fugate                     | St. Teresa's Academy |    24.45 [Individual]\n",
            "\n",
            "100 Fly:\n",
            "--------------------------------------------------------------------------------\n",
            "  Rank   1 | Charlotte Brown                | Villa Duchesne       |    56.48 [Individual]\n",
            "  Rank   1 | Helena Tietjen                 | Pembroke Hill        |    56.64 [Individual]\n",
            "  Rank   2 | Paige Blom                     | Marquette            |    56.86 [Individual]\n",
            "  Rank   3 | Jenna Ridings                  | Pattonville          |    56.87 [Individual]\n",
            "  Rank   4 | Avery Cunneen                  | St. Joseph's Academy |    57.10 [Individual]\n",
            "\n",
            "100 Free:\n",
            "--------------------------------------------------------------------------------\n",
            "  Rank   1 | Avery Cunneen                  | St. Joseph's Academy |    51.76 [Individual]\n",
            "  Rank   2 | Helena Tietjen                 | Pembroke Hill        |    51.94 [Individual]\n",
            "  Rank   3 | Anna-Grace Guenther            | St. Joseph's Academy |    52.03 [Individual]\n",
            "  Rank   4 | Paige Blom                     | Marquette            |    52.29 [Individual]\n",
            "  Rank   1 | Avery Frick                    | St. Michael the Arch |    52.41 [Individual]\n",
            "\n",
            "500 Free:\n",
            "--------------------------------------------------------------------------------\n",
            "  Rank   1 | Alexis Cook                    | Parkway South        |  5:00.69 [Individual]\n",
            "  Rank   2 | Avery Hogue                    | Park Hill South      |  5:01.76 [Individual]\n",
            "  Rank   3 | Madison Shryock                | Rock Bridge          |  5:03.71 [Individual]\n",
            "  Rank   4 | Amelia Coy                     | Hickman              |  5:04.31 [Individual]\n",
            "  Rank   5 | Anna-Grace Guenther            | St. Joseph's Academy |  5:06.93 [Individual]\n",
            "\n",
            "200 Free Relay:\n",
            "--------------------------------------------------------------------------------\n",
            "  Rank   1 | Park Hill South                | Park Hill South      |  1:39.15 [Relay]\n",
            "  Rank   2 | St. Joseph's Academy           | St. Joseph's Academy |  1:40.69 [Relay]\n",
            "  Rank   1 | St. Teresa's Academy           | St. Teresa's Academy |  1:41.10 [Relay]\n",
            "  Rank   3 | Parkway South                  | Parkway South        |  1:41.22 [Relay]\n",
            "  Rank   4 | Pembroke Hill                  | Pembroke Hill        |  1:41.31 [Relay]\n",
            "\n",
            "100 Back:\n",
            "--------------------------------------------------------------------------------\n",
            "  Rank   1 | Avery Cunneen                  | St. Joseph's Academy |    55.26 [Individual]\n",
            "  Rank   2 | Alexis Cook                    | Parkway South        |    56.07 [Individual]\n",
            "  Rank   1 | Karsynne McAlister             | Central (Cape Girard |    56.30 [Individual]\n",
            "  Rank   2 | Gracey MacLaughlin             | Louisiana            |    57.04 [Individual]\n",
            "  Rank   3 | Elizabeth Landuyt              | Park Hill South      |    57.35 [Individual]\n",
            "\n",
            "100 Breast:\n",
            "--------------------------------------------------------------------------------\n",
            "  Rank   1 | Helena Tietjen                 | Pembroke Hill        |  1:03.80 [Individual]\n",
            "  Rank   2 | Sarah Miner                    | Rock Bridge          |  1:05.78 [Individual]\n",
            "  Rank   3 | Payton Robic                   | St. Joseph's Academy |  1:06.30 [Individual]\n",
            "  Rank   1 | Lily Kohler                    | Mehlville            |  1:06.84 [Individual]\n",
            "  Rank   2 | Riley Muran                    | Parkway Central      |  1:07.62 [Individual]\n",
            "\n",
            "400 Free Relay:\n",
            "--------------------------------------------------------------------------------\n",
            "  Rank   1 | St. Joseph's Academy           | St. Joseph's Academy |  3:34.94 [Relay]\n",
            "  Rank   2 | Park Hill South                | Park Hill South      |  3:36.63 [Relay]\n",
            "  Rank   3 | Kirkwood                       | Kirkwood             |  3:36.96 [Relay]\n",
            "  Rank   4 | Rock Bridge                    | Rock Bridge          |  3:37.18 [Relay]\n",
            "  Rank   5 | Parkway South                  | Parkway South        |  3:39.02 [Relay]\n",
            "\n",
            "================================================================================\n",
            "SUMMARY STATISTICS\n",
            "================================================================================\n",
            "Total records: 1517\n",
            "Relay records: 269\n",
            "Individual records: 1248\n",
            "\n",
            "Unique swimmers: 401\n",
            "Unique teams: 106\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "\n",
        "# List of events in the exact order they appear on the page\n",
        "EVENT_ORDER = [\n",
        "    \"200 Medley Relay\",\n",
        "    \"200 Free\",\n",
        "    \"200 IM\",\n",
        "    \"50 Free\",\n",
        "    \"100 Fly\",\n",
        "    \"100 Free\",\n",
        "    \"500 Free\",\n",
        "    \"200 Free Relay\",\n",
        "    \"100 Back\",\n",
        "    \"100 Breast\",\n",
        "    \"400 Free Relay\"\n",
        "]\n",
        "\n",
        "def clean_time(time_str):\n",
        "    \"\"\"Remove * (record marker) and normalize whitespace so times don't break CSV.\"\"\"\n",
        "    if not time_str:\n",
        "        return time_str\n",
        "    # cleaned = time_str.replace('*', '').replace('\\n', ' ').replace('\\r', ' ')\n",
        "    cleaned = time_str.replace('*', '')\n",
        "    return ' '.join(cleaned.split()).strip()\n",
        "\n",
        "def scrape_all_events_with_known_order(html_file_path, output_csv_path):\n",
        "    \"\"\"\n",
        "    Scrape all swimming events from the HTML file\n",
        "    Uses the known event order from EVENT_ORDER list\n",
        "    \"\"\"\n",
        "    with open(html_file_path, 'r', encoding='utf-8') as file:\n",
        "        html_content = file.read()\n",
        "    \n",
        "    soup = BeautifulSoup(html_content, 'html.parser')\n",
        "    all_data = []\n",
        "    \n",
        "    # Find all event tables (fs_grid elements)\n",
        "    fs_grid_tables = soup.find_all('table', class_='fs_grid')\n",
        "    \n",
        "    print(f\"Found {len(fs_grid_tables)} event tables\")\n",
        "    print(f\"Expected {len(EVENT_ORDER)} events based on your list\")\n",
        "    \n",
        "    # Make sure there's the right number of tables\n",
        "    if len(fs_grid_tables) != len(EVENT_ORDER):\n",
        "        print(f\"Warning: Found {len(fs_grid_tables)} tables but expected {len(EVENT_ORDER)} events\")\n",
        "        print(\"Using available events in order...\")\n",
        "    \n",
        "    # Process each fs_grid table and assign event names from the list\n",
        "    for table_index, table in enumerate(fs_grid_tables):\n",
        "        # Get event name from the predefined list\n",
        "        if table_index < len(EVENT_ORDER):\n",
        "            event_name = EVENT_ORDER[table_index]\n",
        "        else:\n",
        "            event_name = f\"Event {table_index + 1}\"\n",
        "        \n",
        "        print(f\"\\nProcessing {event_name} (table {table_index + 1}/{len(fs_grid_tables)})\")\n",
        "        \n",
        "        # Get all rows in this table\n",
        "        rows = table.find_all('tr')\n",
        "        print(f\"  Table has {len(rows)} rows (including header)\")\n",
        "        \n",
        "        # Determine if this event is a relay based on event name\n",
        "        is_relay_event = \"Relay\" in event_name\n",
        "        \n",
        "        # Skip the header row\n",
        "        for row_index, row in enumerate(rows):\n",
        "            if row_index == 0:  # Skip header row\n",
        "                continue\n",
        "            \n",
        "            tds = row.find_all('td')\n",
        "            if not tds:  # Skip empty rows\n",
        "                continue\n",
        "            \n",
        "            # Determine if this is a relay table based on structure\n",
        "            is_relay_table = len(tds) == 5  # Relay tables have 5 columns\n",
        "            \n",
        "            # Validate: Event name should match table structure\n",
        "            if is_relay_event != is_relay_table:\n",
        "                print(f\"  Warning: Event '{event_name}' structure mismatch!\")\n",
        "                print(f\"    Event suggests relay: {is_relay_event}\")\n",
        "                print(f\"    Table structure suggests relay: {is_relay_table}\")\n",
        "            \n",
        "            if is_relay_table:\n",
        "                # Relay structure: Rank, School, Contest, Date, Time\n",
        "                seed_rank = tds[0].text.strip().replace('#', '')\n",
        "                team = tds[1].text.strip()\n",
        "                \n",
        "                # For relays, name is the same as team\n",
        "                name = team\n",
        "                \n",
        "                # Extract contest/meet\n",
        "                contest = tds[2].text.strip() if len(tds) > 2 else \"\"\n",
        "                \n",
        "                # Extract date\n",
        "                date = tds[3].text.strip() if len(tds) > 3 else \"\"\n",
        "                \n",
        "                # Extract time\n",
        "                best_time = tds[4].text.strip() if len(tds) > 4 else \"\"\n",
        "                \n",
        "                is_relay = True\n",
        "                grade = \"\"  # No grade for relays\n",
        "                \n",
        "            else:\n",
        "                # Individual event structure: Rank, Student, Grade, School, Contest, Date, Time\n",
        "                seed_rank = tds[0].text.strip().replace('#', '')\n",
        "                \n",
        "                # Name and team (mobile version in td[1])\n",
        "                name_cell = tds[1]\n",
        "                name_text = name_cell.get_text(separator='\\n', strip=True)\n",
        "                \n",
        "                # Extract name (first line)\n",
        "                lines = [line.strip() for line in name_text.split('\\n') if line.strip()]\n",
        "                if lines:\n",
        "                    name = lines[0]\n",
        "                else:\n",
        "                    name = name_cell.text.strip()\n",
        "                \n",
        "                # Grade\n",
        "                grade = tds[2].text.strip() if len(tds) > 2 else \"\"\n",
        "                \n",
        "                # Team (desktop version from td[3])\n",
        "                team = tds[3].text.strip() if len(tds) > 3 else \"\"\n",
        "                \n",
        "                # If team is empty, try to extract from name cell (mobile version)\n",
        "                if not team and len(lines) > 1:\n",
        "                    team = lines[1]\n",
        "                \n",
        "                # Extract contest/meet\n",
        "                contest = tds[4].text.strip() if len(tds) > 4 else \"\"\n",
        "                \n",
        "                # Extract date\n",
        "                date = tds[5].text.strip() if len(tds) > 5 else \"\"\n",
        "                \n",
        "                # Extract time\n",
        "                best_time = tds[6].text.strip() if len(tds) > 6 else \"\"\n",
        "                \n",
        "                is_relay = False\n",
        "            \n",
        "            # Clean up name (remove extra whitespace)\n",
        "            name = ' '.join(name.split())\n",
        "            team = ' '.join(team.split())\n",
        "            best_time = clean_time(best_time)\n",
        "            \n",
        "            # Create data record\n",
        "            record = {\n",
        "                'name': name,\n",
        "                'team': team,\n",
        "                'event': event_name,\n",
        "                'best_time': best_time,\n",
        "                'seed_rank': seed_rank,\n",
        "                'is_relay': is_relay,\n",
        "                'grade': grade,\n",
        "                'contest': contest,\n",
        "                'date': date,\n",
        "                'table_index': table_index + 1\n",
        "            }\n",
        "            \n",
        "            all_data.append(record)\n",
        "    \n",
        "    # Convert to DataFrame\n",
        "    df = pd.DataFrame(all_data)\n",
        "    \n",
        "    # Reorder columns to match your requested format\n",
        "    df = df[['name', 'team', 'event', 'best_time', 'seed_rank', 'is_relay']]\n",
        "    \n",
        "    # Write to CSV\n",
        "    df.to_csv(output_csv_path, index=False)\n",
        "    print(f\"\\nScraped {len(all_data)} total records\")\n",
        "    print(f\"Saved to {output_csv_path}\")\n",
        "    \n",
        "    return df\n",
        "\n",
        "def verify_event_counts(df):\n",
        "    \"\"\"\n",
        "    Verify each event has the right number of records\n",
        "    \"\"\"\n",
        "    print(\"\\n=== Event Count Verification ===\")\n",
        "    \n",
        "    event_counts = df.groupby('event').size().reset_index(name='count')\n",
        "    event_counts['is_relay'] = df.groupby('event')['is_relay'].first().values\n",
        "    \n",
        "    # Add expected order\n",
        "    event_order_dict = {event: i for i, event in enumerate(EVENT_ORDER)}\n",
        "    event_counts['order'] = event_counts['event'].map(event_order_dict)\n",
        "    event_counts = event_counts.sort_values('order')\n",
        "    \n",
        "    print(\"Records per event:\")\n",
        "    for _, row in event_counts.iterrows():\n",
        "        relay_flag = \"(Relay)\" if row['is_relay'] else \"(Individual)\"\n",
        "        print(f\"  {row['event']:20} {relay_flag:12} {row['count']:4} records\")\n",
        "    \n",
        "    total_records = event_counts['count'].sum()\n",
        "    print(f\"\\nTotal records across all events: {total_records}\")\n",
        "    \n",
        "    return event_counts\n",
        "\n",
        "def display_sample_data(df, num_samples=5):\n",
        "    \"\"\"\n",
        "    Display sample data from each event\n",
        "    \"\"\"\n",
        "    print(\"\\n=== Sample Data from Each Event ===\")\n",
        "    \n",
        "    for event in EVENT_ORDER:\n",
        "        if event in df['event'].values:\n",
        "            event_data = df[df['event'] == event]\n",
        "            print(f\"\\n{event}:\")\n",
        "            print(\"-\" * 80)\n",
        "            \n",
        "            samples = event_data.head(num_samples)\n",
        "            for _, row in samples.iterrows():\n",
        "                relay_flag = \"[Relay]\" if row['is_relay'] else \"[Individual]\"\n",
        "                print(f\"  Rank {row['seed_rank']:>3} | {row['name'][:30]:30} | \"\n",
        "                      f\"{row['team'][:20]:20} | {row['best_time']:>8} {relay_flag}\")\n",
        "\n",
        "def main():\n",
        "    html_file = \"MSHSAA Swimming Performance List.html\"\n",
        "    output_file = \"swimming_performance.csv\"\n",
        "    \n",
        "    print(\"=\" * 80)\n",
        "    print(\"MSHSAA Swimming Performance Scraper\")\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"Processing: {html_file}\")\n",
        "    print(f\"Output: {output_file}\")\n",
        "    print(f\"\\nEvent List ({len(EVENT_ORDER)} events):\")\n",
        "    for i, event in enumerate(EVENT_ORDER, 1):\n",
        "        relay_flag = \"(Relay)\" if \"Relay\" in event else \"(Individual)\"\n",
        "        print(f\"  {i:2}. {event:20} {relay_flag}\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"Starting scrape...\")\n",
        "    \n",
        "    # Run the scraper\n",
        "    df = scrape_all_events_with_known_order(html_file, output_file)\n",
        "    \n",
        "    # Verify results\n",
        "    event_counts = verify_event_counts(df)\n",
        "    \n",
        "    # Display samples\n",
        "    display_sample_data(df)\n",
        "    \n",
        "    # Summary statistics\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"SUMMARY STATISTICS\")\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"Total records: {len(df)}\")\n",
        "    print(f\"Relay records: {df['is_relay'].sum()}\")\n",
        "    print(f\"Individual records: {len(df) - df['is_relay'].sum()}\")\n",
        "    print(f\"\\nUnique swimmers: {df[~df['is_relay']]['name'].nunique()}\")\n",
        "    print(f\"Unique teams: {df['team'].nunique()}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b0fa1ff",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "4d2bf5b0",
      "metadata": {},
      "source": [
        "# Import dive performance data\n",
        "\n",
        "Parse MSHSAA Dive Performance Listing.html: two tables (Class 1, Class 2). Output `dive_performance.csv` and optionally `class1_dive.csv` / `class2_dive.csv`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "846dd414",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved 94 dive records to dive_performance.csv\n",
            "  Class 1: 41, Class 2: 53\n",
            "  class1_dive.csv: 41 rows\n",
            "  class2_dive.csv: 53 rows\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>name</th>\n",
              "      <th>team</th>\n",
              "      <th>total_score</th>\n",
              "      <th>difficulty</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Naya Narciso</td>\n",
              "      <td>Notre Dame de Sion</td>\n",
              "      <td>418.4011</td>\n",
              "      <td>22.6</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Journey Wildschuetz</td>\n",
              "      <td>Central (Springfield)</td>\n",
              "      <td>404.2511</td>\n",
              "      <td>22.3</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Stella Henderson</td>\n",
              "      <td>Notre Dame de Sion</td>\n",
              "      <td>379.7511</td>\n",
              "      <td>21.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Clara Hugge</td>\n",
              "      <td>Visitation Academy</td>\n",
              "      <td>369.2011</td>\n",
              "      <td>20.1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Ella Ball</td>\n",
              "      <td>Father Tolton</td>\n",
              "      <td>361.8011</td>\n",
              "      <td>21.2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Hayden Duffy</td>\n",
              "      <td>Visitation Academy</td>\n",
              "      <td>349.4511</td>\n",
              "      <td>20.7</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Gabriella Pupillo</td>\n",
              "      <td>Parkway West</td>\n",
              "      <td>334.1511</td>\n",
              "      <td>20.2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Evelyn Behrens</td>\n",
              "      <td>Lincoln College Prep</td>\n",
              "      <td>332.3511</td>\n",
              "      <td>21.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Eloise Sweeny</td>\n",
              "      <td>St. Teresa's Academy</td>\n",
              "      <td>330.8511</td>\n",
              "      <td>20.2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Grace Gunter</td>\n",
              "      <td>St. Teresa's Academy</td>\n",
              "      <td>326.6011</td>\n",
              "      <td>20.8</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                  name                   team  total_score  difficulty  class\n",
              "0         Naya Narciso     Notre Dame de Sion     418.4011        22.6      1\n",
              "1  Journey Wildschuetz  Central (Springfield)     404.2511        22.3      1\n",
              "2     Stella Henderson     Notre Dame de Sion     379.7511        21.0      1\n",
              "3          Clara Hugge     Visitation Academy     369.2011        20.1      1\n",
              "4            Ella Ball          Father Tolton     361.8011        21.2      1\n",
              "5         Hayden Duffy     Visitation Academy     349.4511        20.7      1\n",
              "6    Gabriella Pupillo           Parkway West     334.1511        20.2      1\n",
              "7       Evelyn Behrens   Lincoln College Prep     332.3511        21.0      1\n",
              "8        Eloise Sweeny   St. Teresa's Academy     330.8511        20.2      1\n",
              "9         Grace Gunter   St. Teresa's Academy     326.6011        20.8      1"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "def scrape_dive_performance(html_path=\"MSHSAA Dive Performance Listing.html\", output_path=\"dive_performance.csv\"):\n",
        "    \"\"\"Parse dive tables from MSHSAA HTML. First table = Class 1, second = Class 2.\"\"\"\n",
        "    with open(html_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        soup = BeautifulSoup(f, \"html.parser\")\n",
        "\n",
        "    tables = soup.find_all(\"table\", class_=\"fs_grid\")\n",
        "    if len(tables) < 2:\n",
        "        print(f\"Warning: expected 2 dive tables (Class 1 & 2), found {len(tables)}\")\n",
        "\n",
        "    rows_all = []\n",
        "    for table_idx, table in enumerate(tables):\n",
        "        class_num = table_idx + 1  # 1 or 2\n",
        "        for tr in table.find_all(\"tr\"):\n",
        "            if \"hide\" in (tr.get(\"class\") or []):\n",
        "                continue\n",
        "            tds = tr.find_all(\"td\")\n",
        "            if len(tds) < 5:\n",
        "                continue\n",
        "            rank_text = tds[0].get_text(strip=True)\n",
        "            if not rank_text or not rank_text.replace(\"#\", \"\").isdigit():\n",
        "                continue  # skip header\n",
        "            # Student: name on first line, school in div.small.gray (or second line)\n",
        "            student_cell = tds[1]\n",
        "            lines = [l.strip() for l in student_cell.get_text(separator=\"\\n\", strip=True).split(\"\\n\") if l.strip()]\n",
        "            name = lines[0] if lines else \"\"\n",
        "            team = lines[1] if len(lines) > 1 else \"\"\n",
        "            if not team:\n",
        "                school_div = student_cell.find(\"div\", class_=lambda c: c and (\"gray\" in (c if isinstance(c, list) else [])))\n",
        "                if school_div:\n",
        "                    team = school_div.get_text(strip=True)\n",
        "            name = \" \".join(name.split())\n",
        "            team = \" \".join(team.split())\n",
        "            # Score: first number in cell (e.g. 418.40)\n",
        "            score_text = tds[3].get_text(strip=True)\n",
        "            m = re.search(r\"[\\d.]+\", score_text)\n",
        "            total_score = float(m.group()) if m else None\n",
        "            if total_score is None:\n",
        "                continue\n",
        "            try:\n",
        "                difficulty = float(tds[4].get_text(strip=True))\n",
        "            except (ValueError, IndexError):\n",
        "                difficulty = None\n",
        "            rows_all.append({\n",
        "                \"name\": name,\n",
        "                \"team\": team,\n",
        "                \"total_score\": total_score,\n",
        "                \"difficulty\": difficulty,\n",
        "                \"class\": class_num,\n",
        "            })\n",
        "\n",
        "    df = pd.DataFrame(rows_all)\n",
        "    df = df.sort_values([\"class\", \"total_score\"], ascending=[True, False]).reset_index(drop=True)\n",
        "    df.to_csv(output_path, index=False)\n",
        "    print(f\"Saved {len(df)} dive records to {output_path}\")\n",
        "    print(f\"  Class 1: {len(df[df['class']==1])}, Class 2: {len(df[df['class']==2])}\")\n",
        "\n",
        "    # Optional: class-specific CSVs\n",
        "    for c in [1, 2]:\n",
        "        subset = df[df[\"class\"] == c]\n",
        "        if not subset.empty:\n",
        "            subset.to_csv(f\"class{c}_dive.csv\", index=False)\n",
        "            print(f\"  class{c}_dive.csv: {len(subset)} rows\")\n",
        "    return df\n",
        "\n",
        "df_dive = scrape_dive_performance()\n",
        "df_dive.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0feaec87",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
